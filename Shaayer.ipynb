{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "d4046c5623614de08c574062b35977d1",
            "7323fa720afe43c49e9f78233d8d66ae"
          ]
        },
        "id": "ZdTkUJ9lY5QI",
        "outputId": "d04beaf4-c106-4778-d9bf-867689367bc6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4046c5623614de08c574062b35977d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rekhta.org Poetry Scraped Successfully!\n",
            "Scraped Poetry Saved Successfully to RekhtaScrapedPoetry.txt\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from collections import deque\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn\n",
        "\n",
        "Base_Url = \"https://www.rekhta.org/\"                                        # Base Url\n",
        "Allowed_Keyword = \"shayari\"                                                 # Only Poetry Sites\n",
        "Excluded_Keyword = \"lang\"                                                   # Only Urdu\n",
        "Max_Pages = 300                                                             # Maximun Pages Crawl\n",
        "\n",
        "# Normalize Url For Identity Keywords\n",
        "def Normalize_Url(Url):\n",
        "    Parsed = urlparse(Url)\n",
        "    Scheme = Parsed.scheme\n",
        "    Netloc = Parsed.netloc.lower()\n",
        "    Path = Parsed.path.rstrip('/')\n",
        "    return f\"{Scheme}://{Netloc}{Path}\"\n",
        "\n",
        "# Extract Poetry After inspection HTML\n",
        "def Extract_Poetry(Url):\n",
        "    Response = requests.get(Url, timeout=10)\n",
        "    if Response.status_code != 200:\n",
        "        return []\n",
        "    soup = BeautifulSoup(Response.content, 'html.parser')\n",
        "    Sher_Sections = soup.find_all('div', class_='sherSection')\n",
        "    Poetry_Data = []\n",
        "    for Section in Sher_Sections:\n",
        "        Roman_Lines = Section.find('div', {'data-roman': 'on'})\n",
        "        if Roman_Lines:\n",
        "            Lines = Roman_Lines.find_all('p')\n",
        "            Sher = []\n",
        "            for Line in Lines:\n",
        "                Spans = Line.find_all('span')\n",
        "                Line_Roman_Text = \" \".join(Span.get_text(strip=True) for Span in Spans)\n",
        "                Sher.append(Line_Roman_Text.strip())\n",
        "            Poetry_Data.append(\"\\n\".join(Sher))\n",
        "    return Poetry_Data\n",
        "\n",
        "# Crawling Websites Function\n",
        "def Crawl_Website(Start_Url):\n",
        "    Normalized_Start = Normalize_Url(Start_Url)\n",
        "    Normalized_Base = Normalize_Url(Base_Url)\n",
        "\n",
        "    Seen = set([Normalized_Start])\n",
        "    Queue = deque([Normalized_Start])\n",
        "    All_Poetry = []\n",
        "    Crawled_Pages = 0\n",
        "\n",
        "    with Progress(\n",
        "        TextColumn(\"Crawling : \"),  # Show crawling URL\n",
        "        BarColumn(),\n",
        "        TextColumn(\"{task.percentage:>3.0f}%\"),\n",
        "    ) as Progress_Indicator:\n",
        "\n",
        "        Progress_Task = Progress_Indicator.add_task(\"Progress\", total=Max_Pages)\n",
        "\n",
        "        while Queue and Crawled_Pages < Max_Pages:\n",
        "            Url = Queue.popleft()\n",
        "            try:\n",
        "                response = requests.get(Url, timeout=10)\n",
        "                if response.status_code != 200:\n",
        "                    continue\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                Poetry_Data = Extract_Poetry(Url)\n",
        "                All_Poetry.extend(Poetry_Data)\n",
        "                Crawled_Pages += 1\n",
        "\n",
        "                # Find All Link on that Page\n",
        "                for Link in soup.find_all('a', href=True):\n",
        "                    Full_Url = urljoin(Url, Link['href'])\n",
        "                    Clean_Url = Normalize_Url(Full_Url)\n",
        "                    # Conditions to follow the link:\n",
        "                    if (\n",
        "                        Clean_Url not in Seen and\n",
        "                        Allowed_Keyword in Clean_Url and\n",
        "                        Excluded_Keyword not in Clean_Url and\n",
        "                        Clean_Url.startswith(Normalized_Base)\n",
        "                    ):\n",
        "                        Seen.add(Clean_Url)\n",
        "                        Queue.append(Clean_Url)\n",
        "                Progress_Indicator.update(Progress_Task, advance=1)\n",
        "            except requests.RequestException as e:\n",
        "                print(f\"Request failed: {Url} - {e}\")\n",
        "    return All_Poetry\n",
        "\n",
        "# Start Crawling\n",
        "All_Poetry = Crawl_Website(Base_Url)\n",
        "\n",
        "# Complete Crawling\n",
        "print(f\"Rekhta.org Poetry Scraped Successfully!\")\n",
        "\n",
        "# Save Scraped Poetry to a .txt File\n",
        "File_Path = \"RekhtaScrapedPoetry.txt\"\n",
        "with open(File_Path, \"w\", encoding=\"utf-8\") as File:\n",
        "    File.write(\"\\n\".join(All_Poetry))\n",
        "\n",
        "print(f\"Scraped Poetry Saved Successfully to {File_Path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl99E26RfpBr",
        "outputId": "7c9886f4-ac98-436a-cb8e-168d691f3a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rekhta.org Poetry Cleaned Successfully!\n",
            "Cleaned Poetry Saved Successfully to RekhtaCleanedPoetry.txt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Clean Scraped Poetry Text\n",
        "def Clean_Poetry(RekhtaScrapedPoetry, RekhtaCleanedPoetry):\n",
        "    with open(RekhtaScrapedPoetry, \"r\", encoding=\"utf-8\") as f:\n",
        "        ScrapedPoetryLines = f.readlines()                                                                  # Read all lines\n",
        "\n",
        "    CleanedPoetry = \" \".join(Line.strip() for Line in ScrapedPoetryLines if Line.strip())                   # Remove lines and Join in Single String\n",
        "    CleanedPoetry = CleanedPoetry.lower()                                                                   # Convert to Lowercase\n",
        "    CleanedPoetry = re.sub(r'\\s+', ' ', CleanedPoetry).strip()                                              # Remove Extra Spaces\n",
        "    CleanedPoetry = re.sub(r'[^\\w\\s]', '', CleanedPoetry)                                                   # Remove Special characters\n",
        "\n",
        "    # Save cleaned text to output file\n",
        "    with open(RekhtaCleanedPoetry, \"w\", encoding=\"utf-8\") as File:\n",
        "        File.write(CleanedPoetry)\n",
        "\n",
        "    print(f\"Rekhta.org Poetry Cleaned Successfully!\")\n",
        "    print(f\"Cleaned Poetry Saved Successfully to {RekhtaCleanedPoetry}\")\n",
        "\n",
        "\n",
        "ScrapedPoetryFile_Path = \"RekhtaScrapedPoetry.txt\";\n",
        "CleanedPoetryFile_Path = \"RekhtaCleanedPoetry.txt\";\n",
        "\n",
        "Clean_Poetry(ScrapedPoetryFile_Path, CleanedPoetryFile_Path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "04004b15b5804f2aad5d37eaae0138cb",
            "edffa00193804ef5be2eee85fe072274"
          ]
        },
        "id": "y_zbnkXYmGjR",
        "outputId": "cf87feb3-3afe-4d4a-b063-b668a6076941"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04004b15b5804f2aad5d37eaae0138cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Unique Words : 1106\n",
            "Max Sequence Length : 50\n",
            "Number of Training Samples : 5000\n",
            "Poetry Tokenizer Saved Successfully to: RomanUrduPoetryTokenizer.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer                                         # type: ignore\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences                                 # type: ignore\n",
        "from rich.progress import Progress, BarColumn, TextColumn\n",
        "\n",
        "with open(\"RekhtaCleanedPoetry.txt\", \"r\", encoding=\"utf-8\") as File:\n",
        "    CleanedPoetry = File.read()                                                                   # Read Cleaned Poetry\n",
        "\n",
        "Maximum_Poetry_Words = 5000 + 1\n",
        "CleanedPoetryWords = CleanedPoetry.split()\n",
        "CleanedPoetry = \" \".join(CleanedPoetryWords[:Maximum_Poetry_Words])\n",
        "\n",
        "# Tokenization\n",
        "PoetryTokenizer = Tokenizer()\n",
        "PoetryTokenizer.fit_on_texts([CleanedPoetry])\n",
        "Total_Words = len(PoetryTokenizer.word_index) + 1                                                 # add 1 for Padding Token\n",
        "\n",
        "# Convert Text to Sequences\n",
        "Input_Sequences = []\n",
        "Words = CleanedPoetry.split()\n",
        "\n",
        "with Progress(\n",
        "        TextColumn(\"Tokenizing : \"),  # Show crawling URL\n",
        "        BarColumn(),\n",
        "        TextColumn(\"{task.percentage:>3.0f}%\"),\n",
        "    ) as Progress_Indicator:\n",
        "        Progress_Task = Progress_Indicator.add_task(\"Progress\", total=len(Words))\n",
        "\n",
        "        for i in range(1, len(Words)):\n",
        "          n_gram_sequence = Words[:i+1]                                                                 # Create N-Grams\n",
        "          encoded = PoetryTokenizer.texts_to_sequences([\" \".join(n_gram_sequence)])[0]\n",
        "          Input_Sequences.append(encoded)\n",
        "          Progress_Indicator.update(Progress_Task, advance=1)\n",
        "\n",
        "# Set Max Sequence Length to 50 | Increase For Better Result\n",
        "Max_Sequence_Length = min(50, max([len(seq) for seq in Input_Sequences]))\n",
        "\n",
        "# Padding Sequences\n",
        "Input_Sequences = pad_sequences(Input_Sequences, maxlen=Max_Sequence_Length, padding=\"pre\")\n",
        "\n",
        "# Split into Features 'X' and Labels 'y'\n",
        "X, Y = Input_Sequences[:, :-1], Input_Sequences[:, -1]\n",
        "Y = np.array(Y)\n",
        "\n",
        "print(f\"Total Unique Words : {Total_Words}\")\n",
        "print(f\"Max Sequence Length : {Max_Sequence_Length}\")\n",
        "print(f\"Number of Training Samples : {len(X)}\")\n",
        "\n",
        "\n",
        "Tokenizer_Path = \"RomanUrduPoetryTokenizer.pkl\"\n",
        "with open(Tokenizer_Path, \"wb\") as File:\n",
        "    pickle.dump(PoetryTokenizer, File)\n",
        "print(f\"Poetry Tokenizer Saved Successfully to: {Tokenizer_Path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_VDBwP-tFAX",
        "outputId": "567a8884-6422-4014-af3a-6c1eeafbda64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 349ms/step - accuracy: 0.0359 - loss: 6.4830\n",
            "Epoch 2/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 340ms/step - accuracy: 0.0423 - loss: 5.8254\n",
            "Epoch 3/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 336ms/step - accuracy: 0.0658 - loss: 5.4406\n",
            "Epoch 4/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 344ms/step - accuracy: 0.1057 - loss: 4.9427\n",
            "Epoch 5/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 340ms/step - accuracy: 0.1869 - loss: 4.1473\n",
            "Epoch 6/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 339ms/step - accuracy: 0.3026 - loss: 3.3608\n",
            "Epoch 7/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 344ms/step - accuracy: 0.4595 - loss: 2.5477\n",
            "Epoch 8/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 345ms/step - accuracy: 0.6203 - loss: 1.8745\n",
            "Epoch 9/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 341ms/step - accuracy: 0.7340 - loss: 1.3611\n",
            "Epoch 10/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 347ms/step - accuracy: 0.8331 - loss: 0.9516\n",
            "Epoch 11/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 341ms/step - accuracy: 0.8991 - loss: 0.6615\n",
            "Epoch 12/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 338ms/step - accuracy: 0.9435 - loss: 0.4448\n",
            "Epoch 13/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 336ms/step - accuracy: 0.9676 - loss: 0.3059\n",
            "Epoch 14/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 338ms/step - accuracy: 0.9832 - loss: 0.1987\n",
            "Epoch 15/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 336ms/step - accuracy: 0.9877 - loss: 0.1449\n",
            "Epoch 16/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 339ms/step - accuracy: 0.9945 - loss: 0.0961\n",
            "Epoch 17/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 338ms/step - accuracy: 0.9938 - loss: 0.0800\n",
            "Epoch 18/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 338ms/step - accuracy: 0.9936 - loss: 0.0651\n",
            "Epoch 19/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 334ms/step - accuracy: 0.9935 - loss: 0.0575\n",
            "Epoch 20/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 334ms/step - accuracy: 0.9959 - loss: 0.0455\n",
            "Epoch 21/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 335ms/step - accuracy: 0.9945 - loss: 0.0414\n",
            "Epoch 22/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 336ms/step - accuracy: 0.9958 - loss: 0.0336\n",
            "Epoch 23/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 344ms/step - accuracy: 0.9943 - loss: 0.0339\n",
            "Epoch 24/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 339ms/step - accuracy: 0.9943 - loss: 0.0295\n",
            "Epoch 25/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 335ms/step - accuracy: 0.9968 - loss: 0.0241\n",
            "Epoch 26/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 339ms/step - accuracy: 0.9956 - loss: 0.0246\n",
            "Epoch 27/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 334ms/step - accuracy: 0.9939 - loss: 0.0262\n",
            "Epoch 28/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 334ms/step - accuracy: 0.9968 - loss: 0.0219\n",
            "Epoch 29/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 343ms/step - accuracy: 0.9957 - loss: 0.0200\n",
            "Epoch 30/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 335ms/step - accuracy: 0.9946 - loss: 0.0232\n",
            "Epoch 31/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 334ms/step - accuracy: 0.9951 - loss: 0.0197\n",
            "Epoch 32/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 338ms/step - accuracy: 0.9947 - loss: 0.0185\n",
            "Epoch 33/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 338ms/step - accuracy: 0.9971 - loss: 0.0146\n",
            "Epoch 34/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 337ms/step - accuracy: 0.9966 - loss: 0.0175\n",
            "Epoch 35/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 337ms/step - accuracy: 0.9968 - loss: 0.0122\n",
            "Epoch 36/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 333ms/step - accuracy: 0.9976 - loss: 0.0127\n",
            "Epoch 37/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 344ms/step - accuracy: 0.9984 - loss: 0.0096\n",
            "Epoch 38/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 336ms/step - accuracy: 0.9965 - loss: 0.0154\n",
            "Epoch 39/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 332ms/step - accuracy: 0.9968 - loss: 0.0181\n",
            "Epoch 40/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 338ms/step - accuracy: 0.9015 - loss: 0.3877\n",
            "Epoch 41/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 335ms/step - accuracy: 0.9200 - loss: 0.3265\n",
            "Epoch 42/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 330ms/step - accuracy: 0.9912 - loss: 0.0563\n",
            "Epoch 43/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 335ms/step - accuracy: 0.9972 - loss: 0.0196\n",
            "Epoch 44/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 333ms/step - accuracy: 0.9979 - loss: 0.0140\n",
            "Epoch 45/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 331ms/step - accuracy: 0.9975 - loss: 0.0134\n",
            "Epoch 46/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 329ms/step - accuracy: 0.9974 - loss: 0.0110\n",
            "Epoch 47/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 328ms/step - accuracy: 0.9983 - loss: 0.0086\n",
            "Epoch 48/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 327ms/step - accuracy: 0.9989 - loss: 0.0085\n",
            "Epoch 49/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 339ms/step - accuracy: 0.9980 - loss: 0.0090\n",
            "Epoch 50/50\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 332ms/step - accuracy: 0.9990 - loss: 0.0066\n",
            "Poetry Generator Model Saved Successfully to: RomanUrduPoetryModel.keras\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential                                                  # type: ignore\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense                                       # type: ignore\n",
        "\n",
        "# Set GRU Model\n",
        "model = Sequential([\n",
        "    Embedding(Total_Words, 100),                          # Word Embeddings\n",
        "    GRU(256, return_sequences=True),                                                            # First GRU layer\n",
        "    GRU(256),                                                                                   # Second GRU layer\n",
        "    Dense(Total_Words, activation=\"softmax\")                                                    # The Output layer with Softmax\n",
        "])\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the Model\n",
        "epochs = 50                                                                                    # Increase for Better Result\n",
        "history = model.fit(X, Y, epochs=epochs, verbose=1)\n",
        "\n",
        "# Save the Model\n",
        "Model_Path = \"RomanUrduPoetryModel.keras\"\n",
        "model.save(Model_Path)\n",
        "\n",
        "print(f\"Poetry Generator Model Saved Successfully to: {Model_Path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv6dDYOj10kJ",
        "outputId": "cafe890f-9338-4053-c915-b9480d96f795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed Text : Dil jo tuta\n",
            "Generated Poetry : Dil jo tuta wahshaten hisse mein apne aai hain ki tere ghar bhi pahunch kar sakun na paen hum kaisa firaq kaisi judai\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences                                 # type: ignore\n",
        "\n",
        "# Load the Poetry Trained Model Form Saved File\n",
        "model = tf.keras.models.load_model(\"RomanUrduPoetryModel.keras\")\n",
        "\n",
        "# Load Poetry Tokenizer From Saved File\n",
        "with open(\"RomanUrduPoetryTokenizer.pkl\", \"rb\") as File:\n",
        "    PoetryTokenizer = pickle.load(File)\n",
        "\n",
        "# Set Max Sequence Length\n",
        "Max_Sequence_Length = 50\n",
        "\n",
        "# Generated Poetry Function\n",
        "def Generate_Poetry(Seed_Text, Next_Words_Length = 20, Temperature = 0.8):\n",
        "    Token_List = PoetryTokenizer.texts_to_sequences([Seed_Text])[0]\n",
        "\n",
        "    for i in range(Next_Words_Length):\n",
        "        Token_List = pad_sequences([Token_List], maxlen=Max_Sequence_Length - 1, padding=\"pre\")\n",
        "\n",
        "        Predictions = model.predict(Token_List, verbose=0)[0]\n",
        "        Predictions = np.log(Predictions + 1e-7) / Temperature\n",
        "        Exp_Preds = np.exp(Predictions)\n",
        "        Probabilities = Exp_Preds / np.sum(Exp_Preds)\n",
        "\n",
        "        Predicted = np.random.choice(len(Probabilities), p=Probabilities)\n",
        "\n",
        "        Output_Poetry_Word = next((Word for Word, index in PoetryTokenizer.word_index.items() if index == Predicted), \"\")\n",
        "        if not Output_Poetry_Word:\n",
        "            break\n",
        "\n",
        "        Seed_Text += \" \" + Output_Poetry_Word\n",
        "        Token_List = np.append(Token_List, Predicted)\n",
        "    return Seed_Text\n",
        "\n",
        "# Get Generated Poetry\n",
        "SeedText = \"Dil jo tuta\"\n",
        "Generated_Poetry = Generate_Poetry(Seed_Text = SeedText, Next_Words_Length = 20, Temperature = 0.8)\n",
        "print(f\"Seed Text : {SeedText}\")\n",
        "print(f\"Generated Poetry : {Generated_Poetry}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhPGWuZ_S3-F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from gradio.themes.base import Base\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences                                 # type: ignore\n",
        "\n",
        "# Load the Trained Model\n",
        "model = tf.keras.models.load_model(\"RomanUrduPoetryModel.keras\")\n",
        "\n",
        "# Load Poetry Tokenizer\n",
        "with open(\"RomanUrduPoetryTokenizer.pkl\", \"rb\") as File:\n",
        "    PoetryTokenizer = pickle.load(File)\n",
        "\n",
        "# Set Max Sequence Length\n",
        "Max_Sequence_Length = 50\n",
        "\n",
        "def Generate_Poetry(Seed_Text, Poetry_Words_Length=20, Temperature=0.5):\n",
        "    Token_List = PoetryTokenizer.texts_to_sequences([Seed_Text])[0]\n",
        "\n",
        "    for i in range(Poetry_Words_Length):\n",
        "\n",
        "        Token_List = pad_sequences([Token_List], maxlen=Max_Sequence_Length - 1, padding=\"pre\")\n",
        "\n",
        "        Predictions = model.predict(Token_List, verbose=0)[0]\n",
        "        Predictions = np.log(Predictions + 1e-7) / Temperature\n",
        "        Exp_Preds = np.exp(Predictions)\n",
        "        Probabilities = Exp_Preds / np.sum(Exp_Preds)\n",
        "\n",
        "        Predicted = np.random.choice(len(Probabilities), p=Probabilities)\n",
        "\n",
        "        output_word = next((word for word, index in PoetryTokenizer.word_index.items() if index == Predicted), \"\")\n",
        "        if not output_word:\n",
        "            break\n",
        "\n",
        "        Seed_Text += \" \" + output_word\n",
        "        Token_List = np.append(Token_List, Predicted)                           # Update token list\n",
        "\n",
        "    # Create Genertated Poetry File\n",
        "    File_Path = \"ShaayerGeneratedPoetry.txt\"\n",
        "    with open(File_Path, \"w\", encoding=\"utf-8\") as File:\n",
        "        File.write(Seed_Text)\n",
        "\n",
        "    return Seed_Text, File_Path\n",
        "\n",
        "# Customize UI\n",
        "class Seafoam(Base):\n",
        "    pass\n",
        "seafoam = Seafoam(font=gr.themes.GoogleFont(\"Plus Jakarta Sans\"))\n",
        "\n",
        "style =\"\"\"\n",
        "    .gradio-primary-button {\n",
        "        background: #007bff;\n",
        "        color: white;\n",
        "        font-weight: bold;\n",
        "        border: none;\n",
        "        border-radius: 20px;\n",
        "    }\n",
        "    .gradio-primary-button:hover {\n",
        "        background: #0056b3;\n",
        "    }\n",
        "    .gradio-dropdown {\n",
        "        background: #00000000;\n",
        "    }\n",
        "    .gradio-secondary-button {\n",
        "        background: transparent;\n",
        "        border: 1.5px solid var(--input-border-color);\n",
        "        font-weight: bold;\n",
        "        border-radius: 20px;\n",
        "    }\n",
        "    .gradio-secondary-button:hover {\n",
        "        background: var(--input-border-color);\n",
        "    }\n",
        "    label.container.show_textbox_border.svelte-173056l textarea.svelte-173056l {\n",
        "        background:transparent;\n",
        "        border-radius: 20px;\n",
        "    }\n",
        "    div.svelte-633qhp {\n",
        "        border-radius: 15px;\n",
        "        overflow-y: hidden;\n",
        "    }\n",
        "    span.svelte-1gfkn6j {\n",
        "        padding-left: 20px,\n",
        "        font-size:16px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .gradio-container.gradio-container-5-16-0 .contain span.svelte-1gfkn6j {\n",
        "        padding-left: 12px;\n",
        "    }\n",
        "    .icon-button-wrapper.hide-top-corner.svelte-1jx2rq3 {\n",
        "        border-radius: 20px;\n",
        "        margin: 5px 6.09px 0px 0px;\n",
        "        padding: 6px 5.5px 5px 5.5px;\n",
        "    }\n",
        "    label.svelte-173056l.svelte-173056l {\n",
        "        display: block;\n",
        "        width: 100%;\n",
        "        padding-left: 10px;\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "# Gradio Interface with Better UI\n",
        "with gr.Blocks(theme=seafoam, css=style) as app:\n",
        "\n",
        "    gr.Markdown(\"# Shaayer\")\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        seed_input = gr.Textbox(label=\"Poetry Seed\", placeholder=\"Enter your poetry seed here ...\")\n",
        "        num_words = gr.Slider(10, 50, step=5, label=\"Number of Words\", value=20)\n",
        "        temp = gr.Slider(0.2, 1.0, step=0.1, label=\"Creativity (Temperature)\", value=0.5)\n",
        "\n",
        "    poetry_output = gr.Textbox(label=\"Generated Poetry\", placeholder=\"Generated Poetry will appear here ...\")\n",
        "    download_btn = gr.DownloadButton(\"Download Generated Poetry\", value=\"generated_poetry.txt\", visible=False, elem_classes=[\"gradio-secondary-button\"])\n",
        "\n",
        "    generate_button = gr.Button(\"Generate\", variant=\"primary\", elem_classes=[\"gradio-primary-button\"])\n",
        "\n",
        "    def generate_download_links(seed_input, num_words, temp):\n",
        "        poetry_output, text_file = Generate_Poetry(seed_input, num_words, temp)\n",
        "        return poetry_output, gr.update(value=text_file, visible=True)\n",
        "\n",
        "    generate_button.click(generate_download_links, inputs=[seed_input, num_words, temp], outputs=[poetry_output, download_btn])\n",
        "\n",
        "app.launch(share=True, inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04004b15b5804f2aad5d37eaae0138cb": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_edffa00193804ef5be2eee85fe072274",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Tokenizing :  <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> 100%\n</pre>\n",
                  "text/plain": "Tokenizing :  \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m 100%\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "7323fa720afe43c49e9f78233d8d66ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4046c5623614de08c574062b35977d1": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7323fa720afe43c49e9f78233d8d66ae",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Crawling :  <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> 100%\n</pre>\n",
                  "text/plain": "Crawling :  \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m 100%\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "edffa00193804ef5be2eee85fe072274": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
